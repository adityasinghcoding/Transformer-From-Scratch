{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Singh | [LinkedIn](https://www.linkedin.com/in/adityasingh2022/) | [GitHub](https://github.com/adityasinghcoding)\n",
    "\n",
    "# __Transformer from Scratch__\n",
    "\n",
    "`Transformer`(transform/translate) helps in understanding real world meaning of the data by transforming the relations(important/priority-wise) w.r.t actual fundamental meaning. \n",
    "<br>___Note :___ Transformer acts as the model inside main model.\n",
    "\n",
    "---\n",
    "### __Components of Transformer__\n",
    "1. ___Encoder Layer___\n",
    "   - Input Embeddings\n",
    "   - Positional Encoding\n",
    "   - Self-Attention Layer\n",
    "   - Feed Forward Network\n",
    "2. ___Decoder Layer___\n",
    "   - Masked Self-Attention\n",
    "   - Encoder-Decoder Attention\n",
    "   - Feed-Forward Network\n",
    "\n",
    "---\n",
    "**Attention**: It finds, measure, evaluate the relation of one word or any data with other data present in the batch/sequence/matrix. \n",
    "There are 2 types of Attentions:\n",
    "- **Self Attention**: Evaluate connection/relation of each word/data. \n",
    "- **Multi Head Attention**: Multiple Self Attentions in parallel, to evaluate complex relations between entities(data).\n",
    "--- \n",
    "### __Architecture of Transformers__\n",
    "#### __Encoder Layer__\n",
    "- **Input** (Data in Vector; Embedding of words)\n",
    "- **Positional Encoding** (Metadata of data order with sine/cosine functions)\n",
    "- **Self Attention Layer** (Relation capturing & evaluation or in technical terms: establishing weights)\n",
    "   - $Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ <br>_Softmax(Sum to 1) is used to normalize the data(scores)._\n",
    "      - _Q, K, V:_ Query, Key, Value\n",
    "      - ${QK^T}$: Compute Scores/Interaction with others.\n",
    "      - V: Vectors. \n",
    "   - $\\sqrt{d_k}$: Fixes large dot products causing gradient issues.\n",
    "- __Feed-Forward Network__ (Simple Neural Network)\n",
    "\n",
    "#### __Decoder Layer__\n",
    "- __Masked-Self Attention__ (Hiding future tokens during training)\n",
    "- __Encoder-Decoder Attention__ (___Q___ from Decoder, ___K & V___ from Encoder. Encoder output used to focus on relevant input parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "   def __init__(self, embed_size, heads):\n",
    "      super(SelfAttention, Self).__init__()\n",
    "      self.embed_size = embed_size # Dimension of input embeddings (e.g., 512)\n",
    "      self.heads= heads # Number of attention heads (e.g., 8)\n",
    "      self.head_dim = embed_size // heads # Dimension per head (e.g., 512/8=64)\n",
    "\n",
    "      # Ensuring embed_size is divisible by the number of heads\n",
    "      assert self.head_dim*heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "\n",
    "      # Linear layers for Q, K, V\n",
    "      # Linear layers to project embeddings into Query (Q), Key (K), Value (V) vectors\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "      # Final linear layer to combine outputs from all heads\n",
    "      self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "   \n",
    "   def forward(self, values, keys, queries, mask=None):\n",
    "      # Get batch size (N) and sequence lengths for values, keys, queries\n",
    "      N = queries.shape[0] #Batch Size\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "      # Spliting embeddings into multiple heads (reshape for parallel computation)\n",
    "      # New shape: (N, seq_len, heads, head_dim)\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      queries = values.reshape(N, query_len, self.heads, self.head_dim)\n",
    "      \n",
    "      # Computing Q, K, V\n",
    "      # Projecting embeddings to Q, K, V using linear layers\n",
    "      values = self.values(values)\n",
    "      keys = self.keys(keys)\n",
    "      queries = self.queries(queries)\n",
    "\n",
    "      # Attention scores: (Q*K^T) / sqrt(d_k)\n",
    "      # Compute attention scores (Q * K^T)\n",
    "      # Einstein summation: (batch, query_len, heads, head_dim) x (batch, key_len, heads, head_dim)\n",
    "      \n",
    "      # Result shape: (batch, heads, query_len, key_len)\n",
    "      energy = torch.eisum(\"nhql, nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
    "\n",
    "      # Applying mask (if provided) to ignore certain positions (e.g., padding or future tokens)\n",
    "      if mask is not None:\n",
    "         # Replace masked positions with -inf\n",
    "         energy = energy.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "      \n",
    "      # Normalizing scores using softmax and scale by sqrt(embed_size) for stability\n",
    "      attention = torch.softmax(energy / (self.embed_size**(0.5)), dim = 3)\n",
    "\n",
    "      # Computing weighted sum of values using attention scores\n",
    "      # Result shape: (batch, query_len, heads, head_dim)\n",
    "      out = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values])\n",
    "      \n",
    "      # Reshaping back to (batch, query_len, embed_size) and pass through final linear layer\n",
    "      out = out.reshape(N, query_len, self.embed_size)\n",
    "      out = self.fc_out(out)\n",
    "      return out\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, embed_size, max_seq_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "\n",
    "      # Creating a matrix of shape (max_seq_len, embed_size) initialized to zeros\n",
    "      pe = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "      # Generating positions from 0 to max_seq_len-1\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "      # Computing divisor term for scaling positional encoding\n",
    "      # Using exp and log to avoid numerical instability\n",
    "      div_term = torch.exp(torch.arange(0, embed_size, 2).float()*(-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "      \n",
    "      # Applying sine to even indices and cosine to odd indices\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # Even positions\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # Odd positions\n",
    "\n",
    "      # Register as a buffer (non-trainable parameter) for saving/loading\n",
    "      self.register_buffer(\"pe\", pe.unsqueeze(0)) # (1, max_seq_len, embed_size)\n",
    "\n",
    "\n",
    "      def forward(self, x):\n",
    "         # Add positional encoding to input embeddings\n",
    "         # x shape: (batch, seq_len, embed_size)\n",
    "         # pe shape: (1, max_seq_len, embed_size) â†’ automatically broadcasted\n",
    "         return x + self.pe[:, :x.shape[1], :]\n",
    "         \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block (Encoder Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "   def __init__(self, embed_size, heads, dropout = 0.1):\n",
    "      super(TransformerBlock, self).__init__()\n",
    "\n",
    "      # Multi-head self-attention layer\n",
    "      self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "      # Layer normalization for stabilizing training\n",
    "      self.norm1 = nn.LayerNorm(embed_size)\n",
    "      self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "       # Feed-forward network (expands and contracts embeddings)\n",
    "      self.ff = nn.Sequential(\n",
    "         nn.Linear(embed_size, 4 * embed_size), # Expand to 4 * embed_size\n",
    "         nn.ReLU(), # Non-linearity\n",
    "         nn.Linear(4 * embed_size, embed_size), # Contract back to embed_size\n",
    "      )\n",
    "\n",
    "      # Dropout for regularization\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "      def forward(self, x, mask = None):\n",
    "         # Step 1: Compute self-attention\n",
    "         attention = self.attention(x, x, x, mask)\n",
    "\n",
    "         # Step 2: Residual connection + layer norm\n",
    "         x = self.norm1(attention + x) # Residual skip connection\n",
    "         x = self.dropout(x)\n",
    "\n",
    "         # Step 3: Feed-forward network\n",
    "         ff = self.ff(x)\n",
    "\n",
    "         # Step 4: Residual connection + layer norm\n",
    "         x = self.norm2(ff + x)\n",
    "         x = self.dropout(x)\n",
    "         return x\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "   def __init__(self, src_vocab_size, embed_size, num_layers, heads, max_seq_len, dropout = 0.1):\n",
    "      super(Transformer, self).__init__()\n",
    "\n",
    "      # Embedding layer to convert token IDs to vectors\n",
    "      self.embed = nn.Embedding(src_vocab_size, embed_size)\n",
    "\n",
    "      # Positional encoding to add sequence information\n",
    "      self.pe = PositionalEncoding(embed_size, max_seq_len)\n",
    "\n",
    "      # Stack multiple transformer blocks (encoder layers)\n",
    "      self.layers = nn.ModuleList([TransformerBlock(embed_size, heads, dropout)\n",
    "      for _ in range (num_layers)\n",
    "      ])\n",
    "\n",
    "      # Final linear layer to project embeddings back to vocabulary size\n",
    "      self.fc_out = nn.Linear(embed_size, src_vocab_size)\n",
    "\n",
    "\n",
    "      def forward(self, x, mask = None):\n",
    "         # Step 1: Convert token IDs to embeddings\n",
    "         x = self.embed(x) # (batch, seq_len) â†’ (batch, seq_len, embed_size)\n",
    "\n",
    "         # Step 2: Add positional encoding\n",
    "         x = self.pe(x)\n",
    "\n",
    "         # Step 3: Pass through each transformer block\n",
    "         for layer in self.layers:\n",
    "            x = layer(x,mask)\n",
    "\n",
    "          # Step 4: Project embeddings to vocabulary logits\n",
    "         x = self.fc_out(x) # (batch, seq_len, vocab_size)\n",
    "         return x\n",
    "          \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m# Vocabulary size (e.g., 10 tokens: 0-9)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Initializing model, loss, and optimizer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[39m=\u001b[39m Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n\u001b[0;32m     10\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# For classification tasks\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[1;34m(self, src_vocab_size, embed_size, num_layers, heads, max_seq_len, dropout)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe \u001b[39m=\u001b[39m PositionalEncoding(embed_size, max_seq_len)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Stack multiple transformer blocks (encoder layers)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([TransformerBlock(embed_size, heads, dropout)\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m \u001b[39mrange\u001b[39;49m (num_layers)\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[39m# Final linear layer to project embeddings back to vocabulary size\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(embed_size, src_vocab_size)\n",
      "Cell \u001b[1;32mIn[17], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe \u001b[39m=\u001b[39m PositionalEncoding(embed_size, max_seq_len)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Stack multiple transformer blocks (encoder layers)\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mModuleList([TransformerBlock(embed_size, heads, dropout)\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (num_layers)\n\u001b[0;32m     14\u001b[0m ])\n\u001b[0;32m     16\u001b[0m \u001b[39m# Final linear layer to project embeddings back to vocabulary size\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_out \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(embed_size, src_vocab_size)\n",
      "Cell \u001b[1;32mIn[16], line 6\u001b[0m, in \u001b[0;36mTransformerBlock.__init__\u001b[1;34m(self, embed_size, heads, dropout)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39msuper\u001b[39m(TransformerBlock, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      5\u001b[0m \u001b[39m# Multi-head self-attention layer\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention \u001b[39m=\u001b[39m SelfAttention(embed_size, heads)\n\u001b[0;32m      8\u001b[0m \u001b[39m# Layer normalization for stabilizing training\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLayerNorm(embed_size)\n",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m, in \u001b[0;36mSelfAttention.__init__\u001b[1;34m(self, embed_size, heads)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, embed_size, heads):\n\u001b[1;32m----> 3\u001b[0m    \u001b[39msuper\u001b[39m(SelfAttention, Self)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed_size \u001b[39m=\u001b[39m embed_size \u001b[39m# Dimension of input embeddings (e.g., 512)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads\u001b[39m=\u001b[39m heads \u001b[39m# Number of attention heads (e.g., 8)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Self' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128 # Dimension of embeddings\n",
    "heads = 8 # Number of attention heads\n",
    "num_layers = 3 # Number of transformer blocks\n",
    "max_seq_len = 10 # Maximum sequence length\n",
    "vocab_size = 10 # Vocabulary size (e.g., 10 tokens: 0-9)\n",
    "\n",
    "# Initializing model, loss, and optimizer\n",
    "model = Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n",
    "criterion = nn.CrossEntropyLoss() # For classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Generating toy data (input and target are the same for a copy task)\n",
    "src = torch.randint(0, vocab_size, (32, max_seq_len)) # Fake input (batch_size=32)\n",
    "trg = src.clone() # Target is same as input (simple copy task)\n",
    "\n",
    "\n",
    "# Traning loop \n",
    "for epoch in range(100):\n",
    "   # Forward pass: compute model predictions\n",
    "   output = model(src) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "   # Compute loss (flatten batch and sequence dimensions for cross-entropy)\n",
    "   loss = criterion(output.view(-1, vocab_size), trg.view(-1))\n",
    "\n",
    "   # Backpropagation\n",
    "   optimizer.zero_grad() # Clear gradients\n",
    "   loss.backward() # Compute gradients\n",
    "   optimizer.step() # Update weights\n",
    "   print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
