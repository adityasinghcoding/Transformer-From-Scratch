{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Singh | [LinkedIn](https://www.linkedin.com/in/adityasingh2022/) | [GitHub](https://github.com/adityasinghcoding)\n",
    "\n",
    "# __Transformer from Scratch__\n",
    "\n",
    "`Transformer`(transform/translate) helps in understanding real world meaning of the data by transforming the relations(important/priority-wise) w.r.t actual fundamental meaning. \n",
    "<br>___Note :___ Transformer acts as the model inside main model.\n",
    "\n",
    "---\n",
    "### __Components of Transformer__\n",
    "1. ___Encoder Layer___\n",
    "   - Input Embeddings\n",
    "   - Positional Encoding\n",
    "   - Self-Attention Layer\n",
    "   - Feed Forward Network\n",
    "2. ___Decoder Layer___\n",
    "   - Masked Self-Attention\n",
    "   - Encoder-Decoder Attention\n",
    "   - Feed-Forward Network\n",
    "\n",
    "---\n",
    "**Attention**: It finds, measure, evaluate the relation of one word or any data with other data present in the batch/sequence/matrix. \n",
    "There are 2 types of Attentions:\n",
    "- **Self Attention**: Evaluate connection/relation of each word/data. \n",
    "- **Multi Head Attention**: Multiple Self Attentions in parallel, to evaluate complex relations between entities(data). Head refers to the attention mechanism.\n",
    "--- \n",
    "### __Architecture of Transformers__\n",
    "#### __Encoder Layer__\n",
    "- **Input** (Tokens/Embedding are the raw input to the Transformer.)\n",
    "   Token's embedding transformed/split into 3 vectors: Q, K, V. Each token has its own Q, K, V.\n",
    "- **Positional Encoding** (Metadata of data order with sine/cosine functions)\n",
    "- **Self Attention Layer** (Relation capturing & evaluation or in technical terms: establishing weights)\n",
    "   - $Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ <br>_Softmax(Sum to 1) is used to normalize the data(scores) & to convert scores to probability distribution._\n",
    "      - _Q, K, V:_ Query, Key, Value\n",
    "      - ${QK^T}$: Compute Scores/Interaction with others. <br>_Represents the strength of attention._\n",
    "      - _V_: Value vectors, Dynamic dictionary. <br>_It contains content/dictionary/data which is linked with Q & K through multiplication helping in translating the relation in final understanding/output(placement of word). V helps in understanding the context of the language._\n",
    "   - $\\sqrt{d_k}$: Fixes large dot products causing gradient issues. </br> ${d_k}$ refers to the dimension of keys.<br>___\"d\"___ is the dimensionality.\n",
    "- __Feed-Forward Network__ (Simple Neural Network)\n",
    "\n",
    "#### __Decoder Layer__\n",
    "- __Masked-Self Attention__ (Hiding future tokens during training)\n",
    "- __Encoder-Decoder Attention__ (___Q___ from Decoder, ___K & V___ from Encoder. Encoder output used to focus on relevant input parts)\n",
    "---\n",
    "### How __Q, K, V__ Works in Transformers:\n",
    "   1. __Input Tokens__ → Projected into __Keys (K)__ and __Values (V)__.\n",
    "      - Think: Each token writes its __data (V)__ and a __label (K)__ into a dictionary.\n",
    "   2. __Queries (Q)__ \"search\" this dictionary by comparing __Q__ to all __Ks__.\n",
    "   3. The best-matching __Ks__ (highest attention scores) retrieve their corresponding __Vs__.\n",
    "   4. The output is a __weighted blend__ of the retrieved __Vs__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transformer Output:\n",
      " [[-1.40276885 -0.33733117 -2.87379079  1.36090018 -0.43000261]]\n"
     ]
    }
   ],
   "source": [
    "Embedding = np.array([[1,2,3,4,5]]) # Vectorized input, (1,5)\n",
    "\n",
    "# Hyperparameters Initialization\n",
    "embedding_dim = len(Embedding)\n",
    "qk_dim = embedding_dim\n",
    "v_dim = embedding_dim\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "def SoftMax(input, axis= -1):\n",
    "   normalized_input = np.exp(input - np.max(input, axis=axis, keepdims= True)) # Subtracting maximum value from input to avoid large exponential number\n",
    "   return normalized_input / normalized_input.sum(axis = -1, keepdims = True) # keepdims = True; is used to maintain the compatibility of the matrix dimension with other dimension.\n",
    "\n",
    "def SelfAttention(Q, K, V):\n",
    "   QK = np.dot(Q, K.T) # scores  = sum(Query x Transposed_Keys)\n",
    "   scaled_QK = QK / np.sqrt(qk_dim) # normalized_scores = scores / sqrt(keys_dimension)\n",
    "   attention_weights = SoftMax(scaled_QK, axis = -1) # weights = SoftMax(normalized_scores, from_last_dimension) // It is used to normalize the scores/input summed to 1 at the same time it converts the scores/input in probabilities .\n",
    "   attention = np.dot(attention_weights, V) # attention = attention_weights x Values\n",
    "\n",
    "   # \"attention\" returned for further processing in model in the next layer.\n",
    "   # \"attention_weights\" returned for understanding how the model is making decisions.\n",
    "   return attention, attention_weights\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# # Transformer Initialization without class \n",
    "# # Initializing the weights of Q, K, V\n",
    "# WQ = np.random.randn(embedding_dim, qk_dim) # 2 parameters in random function are for matrix dimensions, ex: (5 x 5) // Note: About 99.7% values ranges from -3 to +3\n",
    "# WK = np.random.randn(embedding_dim, qk_dim) \n",
    "# WV = np.random.randn(embedding_dim, qk_dim)\n",
    "\n",
    "# # Computing Q, K, V from input embedding. In simple words multiplying weights(WQ, WK, WV) with inputs(Embeddings).\n",
    "# Q = np.dot(Embedding, WQ)\n",
    "# K = np.dot(Embedding, WK)\n",
    "# V = np.dot(Embedding, WV)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Transformer Initialization with class\n",
    "class TransformerLayer:\n",
    "   def __init__(self, dim):\n",
    "      # Layer specific parameters\n",
    "      self.WQ = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "      self.WK = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "      self.WV = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "\n",
    "      # Residual connections help gradients flow through deep networks\n",
    "      self.normalize = Layer_Normalization(dim)\n",
    "\n",
    "   def __call__(self, input):\n",
    "      residual = input\n",
    "      Q = np.dot(input, self.WQ)\n",
    "      K = np.dot(input, self.WK)\n",
    "      V = np.dot(input, self.WV)\n",
    "\n",
    "      # Calculating initial attention\n",
    "      attention, _ = SelfAttention(Q, K, V)\n",
    "      \n",
    "      # Adding residual connection (original input) + applying layer normalization\n",
    "      return self.normalize.forward(attention + residual)\n",
    "       \n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Feed Forward Neural Network without Class implementation\n",
    "# He Initialization. It preserve the variance across layers & used for ReLu network to maintain stable gradients\n",
    "def initializing_ffn(input_features, neurons, output_predicted):\n",
    "   '''\n",
    "   Variance = 2 / input_dim\n",
    "   Standard deviation = sqrt(variance)\n",
    "\n",
    "   This ensures the output of each layer has approx ~1 variance, which mitigate gradient issues in training.\n",
    "   '''\n",
    "   w1 = np.random.randn(input_features, neurons) * np.sqrt(2./input_features) # sqrt(2./input_dim) scales the random values.\n",
    "   b1 = np.zeros(neurons)\n",
    "   w2 = np.random.randn(neurons, output_predicted) * np.sqrt(2./neurons)\n",
    "   b2 = np.zeros(output_predicted)\n",
    "   return w1, b1, w2, b2\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# Feed Forward Neural Network with Class implementation\n",
    "class FeedForwardNetwork:\n",
    "   def __init__(self, input_features, neurons, output_predicted):\n",
    "      # HE Initialization\n",
    "      self.w1 = np.random.randn(input_features, neurons) * np.sqrt(2./input_features)\n",
    "      self.b1 = np.zeros(neurons)\n",
    "\n",
    "      self.w2 = np.random.randn(neurons, output_predicted) * np.sqrt(2./neurons)\n",
    "      self.b2 = np.zeros(output_predicted)\n",
    "\n",
    "   # Decoder\n",
    "   def forward_pass(self, input):\n",
    "      # First linear neural layer with activation function ReLU. \n",
    "      self.neurons = np.maximum(0,np.dot(input,self.w1)+ self.b1)\n",
    "\n",
    "      # 2nd(final) linear layer output without any activation function.\n",
    "      output = np.dot(self.neurons,self.w2)+ self.b2\n",
    "      return output\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Positional Encoding (Sinusoidal)\n",
    "# Helping Transformer to understand word order, position information of embeddings.\n",
    "def positional_encoding(sequence_length, embedding_dim):\n",
    "   # Creating position indices (0, 1, 2,... seq_len-1)\n",
    "   position = np.arange(sequence_length)[:, np.newaxis] # Convert to column vector\n",
    "   \n",
    "   # Calculating no. of frequency pairs needed\n",
    "   num_pairs = embedding_dim // 2\n",
    "\n",
    "   # Calculating division term for sinusodial functions\n",
    "   # Creating alternating sine/cosine pattern across dimensions\n",
    "   i = np.arange(num_pairs) # Handles both even & odd patterns\n",
    "   div_term = np.exp(i * (-np.log(10000.0) * 2 / embedding_dim))\n",
    "\n",
    "   # Initializing empty positional encoding matrix\n",
    "   pe = np.zeros((sequence_length, embedding_dim))\n",
    "\n",
    "   # Calculating indices explicitly/clearly/openly\n",
    "   even_indices = 2 * i\n",
    "   odd_indices = 2 * i + 1\n",
    "\n",
    "   # Filling even indices with sine values\n",
    "   pe[:, even_indices] = np.sin(position * div_term) # 0::2 mean start at 0 with stepping size of 2\n",
    "\n",
    "   # Filling odd indices with cosine values\n",
    "   pe[:, odd_indices] = np.cos(position * div_term) # 1::2 mean start at 1 with step size of 2\n",
    "\n",
    "   return pe\n",
    "\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# LAYER NORMALIZATION\n",
    "# Stablizing network by normalization values in each layer\n",
    "class Layer_Normalization:\n",
    "   def __init__(self, dim):\n",
    "      # Learnable parameters - network will adjust these during training\n",
    "      self.gamma = np.ones(dim) # Scaling factor (intitally no scaling)\n",
    "      self.beta = np.zeros(dim) # Shifting factor (initially no shift)\n",
    "   \n",
    "   def forward(self, input):\n",
    "      # Calculating mean & standard deviation across features/input (last dimension)\n",
    "      mean = input.mean(axis = -1, keepdims = True) # Keeping the dimensions for broadcasting\n",
    "      standard_deviation = input.std(axis = -1, keepdims = True)\n",
    "\n",
    "      # Normalizing: (input - mean) / standard_deviation, & then scale/shift\n",
    "      # 1e-6: 1 x 10^(-6) or 0.000001, which prevents/protects from division by zero\n",
    "      return self.gamma * (input - mean) / (standard_deviation + 1e-6) + self.beta       \n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''   \n",
    "\n",
    "# TRANSFORMER LAYER STACKING\n",
    "# Reapeating/Iterating 6 times as in original paper of Transformer\n",
    "# for _ in range(6):\n",
    "#    # Storing previous output for residual connection\n",
    "#    residual = attention\n",
    "\n",
    "#    # Self Attention using current representation\n",
    "#    attention, _ = SelfAttention(attention, attention, attention)\n",
    "\n",
    "#    # Adding residual connection + normalize\n",
    "#    attention = Layer_Normalization(attention + residual)\n",
    "\n",
    "# Creating stacks of transformer layers\n",
    "transformer_layers = [TransformerLayer(5) for _ in range (6)]\n",
    "\n",
    "# Processing through layers\n",
    "attention = Embedding.copy() # Copying original embedding to create the attention input\n",
    "for layer in transformer_layers: # Looping/automating over all layers of the transformer\n",
    "   attention = layer(attention) # Applying each transformer layer to the attention\n",
    "\n",
    "\n",
    "# Adding positional encoding to original embedding\n",
    "sequence_length = 1 # Number of token(s)\n",
    "pe = positional_encoding(1,5)\n",
    "# Embedding += positional_encoding(sequence_length, embedding_dim) # Combining content with position info \n",
    "Embedding_Copy = Embedding + pe\n",
    "\n",
    "# Using FFN (with class)\n",
    "ffn = FeedForwardNetwork(input_features = 5, neurons = 2048, output_predicted = 5)\n",
    "ffn_output = ffn.forward_pass(attention)\n",
    "print(\"\\nTransformer Output:\\n\",ffn_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "   def __init__(self, embed_size, heads):\n",
    "      super(SelfAttention, self).__init__()\n",
    "      self.embed_size = embed_size # Dimension of input embeddings (e.g., 512)\n",
    "      self.heads= heads # Number of attention heads (e.g., 8)\n",
    "      self.head_dim = embed_size // heads # Dimension per head (e.g., 512/8=64)\n",
    "\n",
    "      # Ensuring embed_size is divisible by the number of heads\n",
    "      assert self.head_dim*heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "\n",
    "      # Linear layers for Q, K, V\n",
    "      # Linear layers to project embeddings into Query (Q), Key (K), Value (V) vectors\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "      # Final linear layer to combine outputs from all heads\n",
    "      self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "   \n",
    "   def forward(self, values, keys, queries, mask=None):\n",
    "      # Get batch size (N) and sequence lengths for values, keys, queries\n",
    "      N = queries.shape[0] #Batch Size\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "      # Spliting embeddings into multiple heads (reshape for parallel computation)\n",
    "      # New shape: (N, seq_len, heads, head_dim)\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      queries = values.reshape(N, query_len, self.heads, self.head_dim)\n",
    "      \n",
    "      # Computing Q, K, V\n",
    "      # Projecting embeddings to Q, K, V using linear layers\n",
    "      values = self.values(values)\n",
    "      keys = self.keys(keys)\n",
    "      queries = self.queries(queries)\n",
    "      ''''\n",
    "      Attention scores: (Q*K^T) / sqrt(d_k)\n",
    "      Compute attention scores (Q * K^T)\n",
    "      Einstein summation: (batch, query_len, heads, head_dim) x (batch, key_len, heads, head_dim)\n",
    "      \n",
    "      nqhd, nkhd -> nhqk; notation to describe the interaction of 2 tensors or more.\n",
    "      (nqhd): Queries, (nkhd): keys, (nhqk): Result/Resulting shape\n",
    "\n",
    "      n:\tBatch size (number of sequences)\n",
    "      q:\tQuery sequence length\n",
    "      k:\tKey sequence length\n",
    "      h:\tNumber of attention heads\n",
    "      d:\tDimension per head (head_dim)\n",
    "\n",
    "      - nqhd,nkhd->nhqk computes all pairwise interactions between queries and keys across batches and heads.\n",
    "      - This is the core step in self-attention to determine how words in a sequence relate to each other.\n",
    "      '''\n",
    "      energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])  \n",
    "\n",
    "      # Applying mask (if provided) to ignore certain positions (e.g., padding or future tokens)\n",
    "      if mask is not None:\n",
    "         # Replace masked positions with -inf\n",
    "         energy = energy.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "      \n",
    "      # Normalizing scores using softmax and scale by sqrt(embed_size) for stability\n",
    "      attention = torch.softmax(energy / (self.embed_size**(0.5)), dim = 3)\n",
    "\n",
    "      # Computing weighted sum of values using attention scores\n",
    "      # Result shape: (batch, query_len, heads, head_dim)\n",
    "      out = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values])\n",
    "      \n",
    "      # Reshaping back to (batch, query_len, embed_size) and pass through final linear layer\n",
    "      out = out.reshape(N, query_len, self.embed_size)\n",
    "      out = self.fc_out(out)\n",
    "      return out\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, embed_size, max_seq_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "\n",
    "      # Creating a matrix of shape (max_seq_len, embed_size) initialized to zeros\n",
    "      pe = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "      # Generating positions from 0 to max_seq_len-1\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "      # Computing divisor term for scaling positional encoding\n",
    "      # Using exp and log to avoid numerical instability\n",
    "      div_term = torch.exp(torch.arange(0, embed_size, 2).float()*(-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "      \n",
    "      # Applying sine to even indices and cosine to odd indices\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # Even positions\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # Odd positions\n",
    "\n",
    "      # Register as a buffer (non-trainable parameter) for saving/loading\n",
    "      self.register_buffer(\"pe\", pe.unsqueeze(0)) # (1, max_seq_len, embed_size)\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      # Add positional encoding to input embeddings\n",
    "      # x shape: (batch, seq_len, embed_size)\n",
    "      # pe shape: (1, max_seq_len, embed_size) → automatically broadcasted\n",
    "      return x + self.pe[:, :x.shape[1], :]\n",
    "         \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block (Encoder Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "   def __init__(self, embed_size, heads, dropout = 0.1):\n",
    "      super(TransformerBlock, self).__init__()\n",
    "\n",
    "      # Multi-head self-attention layer\n",
    "      self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "      # Layer normalization for stabilizing training\n",
    "      self.norm1 = nn.LayerNorm(embed_size)\n",
    "      self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "       # Feed-forward network (expands and contracts embeddings)\n",
    "      self.ff = nn.Sequential(\n",
    "         nn.Linear(embed_size, 4 * embed_size), # Expand to 4 * embed_size\n",
    "         nn.ReLU(), # Non-linearity\n",
    "         nn.Linear(4 * embed_size, embed_size), # Contract back to embed_size\n",
    "      )\n",
    "\n",
    "      # Dropout for regularization\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Compute self-attention\n",
    "      attention = self.attention(x, x, x, mask)\n",
    "\n",
    "      # Step 2: Residual connection + layer norm\n",
    "      x = self.norm1(attention + x) # Residual skip connection\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # Step 3: Feed-forward network\n",
    "      ff = self.ff(x)\n",
    "\n",
    "      # Step 4: Residual connection + layer norm\n",
    "      x = self.norm2(ff + x)\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "   def __init__(self, src_vocab_size, embed_size, num_layers, heads, max_seq_len, dropout = 0.1):\n",
    "      super(Transformer, self).__init__()\n",
    "\n",
    "      # Embedding layer to convert token IDs to vectors\n",
    "      self.embed = nn.Embedding(src_vocab_size, embed_size)\n",
    "\n",
    "      # Positional encoding to add sequence information\n",
    "      self.pe = PositionalEncoding(embed_size, max_seq_len)\n",
    "\n",
    "      # Stack multiple transformer blocks (encoder layers)\n",
    "      self.layers = nn.ModuleList([TransformerBlock(embed_size, heads, dropout)\n",
    "      for _ in range (num_layers)\n",
    "      ])\n",
    "\n",
    "      # Final linear layer to project embeddings back to vocabulary size\n",
    "      self.fc_out = nn.Linear(embed_size, src_vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Convert token IDs to embeddings\n",
    "      x = self.embed(x) # (batch, seq_len) → (batch, seq_len, embed_size)\n",
    "\n",
    "      # Step 2: Add positional encoding\n",
    "      x = self.pe(x)\n",
    "\n",
    "      # Step 3: Pass through each transformer block\n",
    "      for layer in self.layers:\n",
    "         x = layer(x,mask)\n",
    "\n",
    "         # Step 4: Project embeddings to vocabulary logits\n",
    "      x = self.fc_out(x) # (batch, seq_len, vocab_size)\n",
    "      return x\n",
    "          \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Output:\n",
      "\n",
      "Epoch 0, Loss: 2.505314826965332\n",
      "Epoch 1, Loss: 1.7464202642440796\n",
      "Epoch 2, Loss: 1.1973932981491089\n",
      "Epoch 3, Loss: 0.7467731237411499\n",
      "Epoch 4, Loss: 0.4208246171474457\n",
      "Epoch 5, Loss: 0.24964067339897156\n",
      "Epoch 6, Loss: 0.14512111246585846\n",
      "Epoch 7, Loss: 0.10315585136413574\n",
      "Epoch 8, Loss: 0.07764178514480591\n",
      "Epoch 9, Loss: 0.06334101408720016\n",
      "Epoch 10, Loss: 0.04884720221161842\n",
      "Epoch 11, Loss: 0.042373571544885635\n",
      "Epoch 12, Loss: 0.03544330224394798\n",
      "Epoch 13, Loss: 0.032105155289173126\n",
      "Epoch 14, Loss: 0.027438536286354065\n",
      "Epoch 15, Loss: 0.025192206725478172\n",
      "Epoch 16, Loss: 0.022478139027953148\n",
      "Epoch 17, Loss: 0.019663888961076736\n",
      "Epoch 18, Loss: 0.01893792673945427\n",
      "Epoch 19, Loss: 0.017982598394155502\n",
      "Epoch 20, Loss: 0.016582416370511055\n",
      "Epoch 21, Loss: 0.01589564047753811\n",
      "Epoch 22, Loss: 0.014367098920047283\n",
      "Epoch 23, Loss: 0.013960190117359161\n",
      "Epoch 24, Loss: 0.013418605551123619\n",
      "Epoch 25, Loss: 0.012272506020963192\n",
      "Epoch 26, Loss: 0.012029631994664669\n",
      "Epoch 27, Loss: 0.011150532402098179\n",
      "Epoch 28, Loss: 0.011008724570274353\n",
      "Epoch 29, Loss: 0.010129425674676895\n",
      "Epoch 30, Loss: 0.010059734806418419\n",
      "Epoch 31, Loss: 0.009729357436299324\n",
      "Epoch 32, Loss: 0.009123293682932854\n",
      "Epoch 33, Loss: 0.008891256526112556\n",
      "Epoch 34, Loss: 0.009055187925696373\n",
      "Epoch 35, Loss: 0.008280591107904911\n",
      "Epoch 36, Loss: 0.00816029217094183\n",
      "Epoch 37, Loss: 0.007709467317909002\n",
      "Epoch 38, Loss: 0.007874506525695324\n",
      "Epoch 39, Loss: 0.0075533040799200535\n",
      "Epoch 40, Loss: 0.007072745356708765\n",
      "Epoch 41, Loss: 0.0073959603905677795\n",
      "Epoch 42, Loss: 0.006919529289007187\n",
      "Epoch 43, Loss: 0.006798988673835993\n",
      "Epoch 44, Loss: 0.006521160248667002\n",
      "Epoch 45, Loss: 0.006596154533326626\n",
      "Epoch 46, Loss: 0.006377365440130234\n",
      "Epoch 47, Loss: 0.006315684411674738\n",
      "Epoch 48, Loss: 0.006092351395636797\n",
      "Epoch 49, Loss: 0.006007381249219179\n",
      "Epoch 50, Loss: 0.005852425005286932\n",
      "Epoch 51, Loss: 0.006111237686127424\n",
      "Epoch 52, Loss: 0.005717207212001085\n",
      "Epoch 53, Loss: 0.005473428405821323\n",
      "Epoch 54, Loss: 0.005750687327235937\n",
      "Epoch 55, Loss: 0.005397511646151543\n",
      "Epoch 56, Loss: 0.005253073759377003\n",
      "Epoch 57, Loss: 0.005248453933745623\n",
      "Epoch 58, Loss: 0.005347011145204306\n",
      "Epoch 59, Loss: 0.005197358317673206\n",
      "Epoch 60, Loss: 0.005025777034461498\n",
      "Epoch 61, Loss: 0.0048675634898245335\n",
      "Epoch 62, Loss: 0.004952990449965\n",
      "Epoch 63, Loss: 0.004793220665305853\n",
      "Epoch 64, Loss: 0.004817486274987459\n",
      "Epoch 65, Loss: 0.004752396605908871\n",
      "Epoch 66, Loss: 0.004701826721429825\n",
      "Epoch 67, Loss: 0.004602540750056505\n",
      "Epoch 68, Loss: 0.004622488282620907\n",
      "Epoch 69, Loss: 0.004320463165640831\n",
      "Epoch 70, Loss: 0.004325956106185913\n",
      "Epoch 71, Loss: 0.004232798237353563\n",
      "Epoch 72, Loss: 0.004273019265383482\n",
      "Epoch 73, Loss: 0.004344017244875431\n",
      "Epoch 74, Loss: 0.004078015219420195\n",
      "Epoch 75, Loss: 0.004061122890561819\n",
      "Epoch 76, Loss: 0.004164443351328373\n",
      "Epoch 77, Loss: 0.0038416702300310135\n",
      "Epoch 78, Loss: 0.004052114672958851\n",
      "Epoch 79, Loss: 0.004077566787600517\n",
      "Epoch 80, Loss: 0.003844537539407611\n",
      "Epoch 81, Loss: 0.003762749955058098\n",
      "Epoch 82, Loss: 0.003737929742783308\n",
      "Epoch 83, Loss: 0.003628042759373784\n",
      "Epoch 84, Loss: 0.0037641164381057024\n",
      "Epoch 85, Loss: 0.0037739332765340805\n",
      "Epoch 86, Loss: 0.0036694910377264023\n",
      "Epoch 87, Loss: 0.003525868058204651\n",
      "Epoch 88, Loss: 0.003538391087204218\n",
      "Epoch 89, Loss: 0.0036087441258132458\n",
      "Epoch 90, Loss: 0.0035113804042339325\n",
      "Epoch 91, Loss: 0.0033739968203008175\n",
      "Epoch 92, Loss: 0.003455245168879628\n",
      "Epoch 93, Loss: 0.0032852175645530224\n",
      "Epoch 94, Loss: 0.003359877271577716\n",
      "Epoch 95, Loss: 0.0031710336916148663\n",
      "Epoch 96, Loss: 0.0031458702869713306\n",
      "Epoch 97, Loss: 0.003186978865414858\n",
      "Epoch 98, Loss: 0.003194965422153473\n",
      "Epoch 99, Loss: 0.0031764719169586897\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128 # Dimension of embeddings\n",
    "heads = 8 # Number of attention heads\n",
    "num_layers = 3 # Number of transformer blocks\n",
    "max_seq_len = 10 # Maximum sequence length\n",
    "vocab_size = 10 # Vocabulary size (e.g., 10 tokens: 0-9)\n",
    "\n",
    "# Initializing model, loss, and optimizer\n",
    "model = Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n",
    "criterion = nn.CrossEntropyLoss() # For classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Generating toy data (input and target are the same for a copy task)\n",
    "src = torch.randint(0, vocab_size, (32, max_seq_len)) # Fake input (batch_size=32)\n",
    "trg = src.clone() # Target is same as input (simple copy task)\n",
    "\n",
    "\n",
    "# Traning loop \n",
    "print(\"Transformer Output:\\n\")\n",
    "for epoch in range(100):\n",
    "   # Forward pass: compute model predictions\n",
    "   output = model(src) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "   # Compute loss (flatten batch and sequence dimensions for cross-entropy)\n",
    "   loss = criterion(output.view(-1, vocab_size), trg.view(-1))\n",
    "\n",
    "   # Backpropagation\n",
    "   optimizer.zero_grad() # Clear gradients\n",
    "   loss.backward() # Compute gradients\n",
    "   optimizer.step() # Update weights\n",
    "   print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
