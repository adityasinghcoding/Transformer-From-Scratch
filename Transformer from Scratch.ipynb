{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Singh | [LinkedIn](https://www.linkedin.com/in/adityasingh2022/) | [GitHub](https://github.com/adityasinghcoding)\n",
    "\n",
    "# __Transformer from Scratch__\n",
    "\n",
    "`Transformer`(transform/translate) helps in understanding real world meaning of the data by transforming the relations(important/priority-wise) w.r.t actual fundamental meaning. \n",
    "<br>___Note :___ Transformer acts as the model inside main model.\n",
    "\n",
    "---\n",
    "### __Components of Transformer__\n",
    "1. ___Encoder Layer___\n",
    "   - Input Embeddings\n",
    "   - Positional Encoding\n",
    "   - Self-Attention Layer\n",
    "   - Feed Forward Network\n",
    "2. ___Decoder Layer___\n",
    "   - Masked Self-Attention\n",
    "   - Encoder-Decoder Attention\n",
    "   - Feed-Forward Network\n",
    "\n",
    "---\n",
    "**Attention**: It finds, measure, evaluate the relation of one word or any data with other data present in the batch/sequence/matrix. \n",
    "There are 2 types of Attentions:\n",
    "- **Self Attention**: Evaluate connection/relation of each word/data. \n",
    "- **Multi Head Attention**: Multiple Self Attentions in parallel, to evaluate complex relations between entities(data). Head refers to the attention mechanism.\n",
    "--- \n",
    "### __Architecture of Transformers__\n",
    "#### __Encoder Layer__\n",
    "- **Input** (Tokens/Embedding are the raw input to the Transformer.)\n",
    "   Token's embedding transformed/split into 3 vectors: Q, K, V. Each token has its own Q, K, V.\n",
    "- **Positional Encoding** (Metadata of data order with sine/cosine functions)\n",
    "- **Self Attention Layer** (Relation capturing & evaluation or in technical terms: establishing weights)\n",
    "   - $Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ <br>_Softmax(Sum to 1) is used to normalize the data(scores) & to convert scores to probability distribution._\n",
    "      - _Q, K, V:_ Query, Key, Value\n",
    "      - ${QK^T}$: Compute Scores/Interaction with others. <br>_Represents the strength of attention._\n",
    "      - _V_: Value vectors, Dynamic dictionary. <br>_It contains content/dictionary/data which is linked with Q & K through multiplication helping in translating the relation in final understanding/output(placement of word). V helps in understanding the context of the language._\n",
    "   - $\\sqrt{d_k}$: Fixes large dot products causing gradient issues. </br> ${d_k}$ refers to the dimension of keys.<br>___\"d\"___ is the dimensionality.\n",
    "- __Feed-Forward Network__ (Simple Neural Network)\n",
    "\n",
    "#### __Decoder Layer__\n",
    "- __Masked-Self Attention__ (Hiding future tokens during training)\n",
    "- __Encoder-Decoder Attention__ (___Q___ from Decoder, ___K & V___ from Encoder. Encoder output used to focus on relevant input parts)\n",
    "---\n",
    "### How __Q, K, V__ Works in Transformers:\n",
    "   1. __Input Tokens__ â†’ Projected into __Keys (K)__ and __Values (V)__.\n",
    "      - Think: Each token writes its __data (V)__ and a __label (K)__ into a dictionary.\n",
    "   2. __Queries (Q)__ \"search\" this dictionary by comparing __Q__ to all __Ks__.\n",
    "   3. The best-matching __Ks__ (highest attention scores) retrieve their corresponding __Vs__.\n",
    "   4. The output is a __weighted blend__ of the retrieved __Vs__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 177\u001b[0m\n\u001b[0;32m    171\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m-------------------------------------------------------------------------------------\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[39m'''\u001b[39;00m   \n\u001b[0;32m    175\u001b[0m \u001b[39m# MODIFIED SELF-ATTENTION BLOCK\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39m# Calculating initial attention\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m attention, _ \u001b[39m=\u001b[39m SelfAttention(Q, K, V)\n\u001b[0;32m    179\u001b[0m \u001b[39m# Adding residual connection (original input) + applying layer normalization\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[39m# Residual connections help gradients flow through deep networks\u001b[39;00m\n\u001b[0;32m    181\u001b[0m layer_normalization \u001b[39m=\u001b[39m Layer_Normalization(embedding_dim)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Q' is not defined"
     ]
    }
   ],
   "source": [
    "Embedding = np.array([[1,2,3,4,5]]) # Vectorized input, (1,5)\n",
    "\n",
    "# Hyperparameters Initialization\n",
    "embedding_dim = len(Embedding)\n",
    "qk_dim = embedding_dim\n",
    "v_dim = embedding_dim\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "def SoftMax(input, axis= -1):\n",
    "   normalized_input = np.exp(input - np.max(input, axis=axis, keepdims= True)) # Subtracting maximum value from input to avoid large exponential number\n",
    "   return normalized_input / normalized_input.sum(axis = -1, keepdims = True) # keepdims = True; is used to maintain the compatibility of the matrix dimension with other dimension.\n",
    "\n",
    "def SelfAttention(Q, K, V):\n",
    "   QK = np.dot(Q, K.T) # scores  = sum(Query x Transposed_Keys)\n",
    "   scaled_QK = QK / np.sqrt(qk_dim) # normalized_scores = scores / sqrt(keys_dimension)\n",
    "   attention_weights = SoftMax(scaled_QK, axis = -1) # weights = SoftMax(normalized_scores, from_last_dimension) // It is used to normalize the scores/input summed to 1 at the same time it converts the scores/input in probabilities .\n",
    "   attention = np.dot(attention_weights, V) # attention = attention_weights x Values\n",
    "\n",
    "   # \"attention\" returned for further processing in model in the next layer.\n",
    "   # \"attention_weights\" returned for understanding how the model is making decisions.\n",
    "   return attention, attention_weights\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# # Transformer Initialization without class \n",
    "# # Initializing the weights of Q, K, V\n",
    "# WQ = np.random.randn(embedding_dim, qk_dim) # 2 parameters in random function are for matrix dimensions, ex: (5 x 5) // Note: About 99.7% values ranges from -3 to +3\n",
    "# WK = np.random.randn(embedding_dim, qk_dim) \n",
    "# WV = np.random.randn(embedding_dim, qk_dim)\n",
    "\n",
    "# # Computing Q, K, V from input embedding. In simple words multiplying weights(WQ, WK, WV) with inputs(Embeddings).\n",
    "# Q = np.dot(Embedding, WQ)\n",
    "# K = np.dot(Embedding, WK)\n",
    "# V = np.dot(Embedding, WV)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Transformer Initialization with class\n",
    "class TransformerLayer:\n",
    "   def __init__(self, dim):\n",
    "      # Layer specific parameters\n",
    "      self.WQ = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "      self.WK = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "      self.WV = np.random.randn(dim, dim) * np.sqrt(2./dim)\n",
    "      self.normalize = Layer_Normalization(dim)\n",
    "\n",
    "   def __call__(self, input):\n",
    "      residual = input\n",
    "      Q = np.dot(input, self.WQ)\n",
    "      K = np.dot(input, self.WK)\n",
    "      V = np.dot(input, self.WV)\n",
    "      attention, _ = SelfAttention(Q, K, V)\n",
    "      \n",
    "      return self.normalize.forward(attention + residual)\n",
    "       \n",
    "\n",
    "        \n",
    "\n",
    "# # Getting Attention & it's weights.\n",
    "# attention, attention_weights = SelfAttention(Q, K, V)\n",
    "# print(\"TRANSFORMER WITHOUT CLASS\")\n",
    "# print(\"Attention:\\n\",attention,\"\\nAttetion Weights:\\n\",attention_weights)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Feed Forward Neural Network without Class implementation\n",
    "# He Initialization. It preserve the variance across layers & used for ReLu network to maintain stable gradients\n",
    "def initializing_ffn(input_features, neurons, output_predicted):\n",
    "   '''\n",
    "   Variance = 2 / input_dim\n",
    "   Standard deviation = sqrt(variance)\n",
    "\n",
    "   This ensures the output of each layer has approx ~1 variance, which mitigate gradient issues in training.\n",
    "   '''\n",
    "   w1 = np.random.randn(input_features, neurons) * np.sqrt(2./input_features) # sqrt(2./input_dim) scales the random values.\n",
    "   b1 = np.zeros(neurons)\n",
    "   w2 = np.random.randn(neurons, output_predicted) * np.sqrt(2./neurons)\n",
    "   b2 = np.zeros(output_predicted)\n",
    "   return w1, b1, w2, b2\n",
    "\n",
    "# Decoder\n",
    "def ffn(input, w1, b1, w2, b2):\n",
    "   neurons = np.maximum(0, np.dot(input, w1) + b1)\n",
    "   output = np.dot(neurons, w2) + b2\n",
    "   return output\n",
    "\n",
    "# # Using FFN (without class)\n",
    "# w1, b1, w2, b2 = initializing_ffn(5, 2048, 5)\n",
    "# output = ffn(attention, w1, b1, w2, b2)\n",
    "# print(\"\\nSimple Transformer Output:\\n\",output)\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# Feed Forward Neural Network with Class implementation\n",
    "class FeedForwardNetwork:\n",
    "   def __init__(self, input_features, neurons, output_predicted):\n",
    "      # HE Initialization\n",
    "      self.w1 = np.random.randn(input_features, neurons) * np.sqrt(2./input_features)\n",
    "      self.b1 = np.zeros(neurons)\n",
    "\n",
    "      self.w2 = np.random.randn(neurons, output_predicted) * np.sqrt(2./neurons)\n",
    "      self.b2 = np.zeros(output_predicted)\n",
    "\n",
    "   def forward_pass(self, input):\n",
    "      # First linear neural layer with activation function ReLU. \n",
    "      self.neurons = np.maximum(0,np.dot(input,self.w1)+ self.b1)\n",
    "\n",
    "      # 2nd(final) linear layer output without any activation function.\n",
    "      output = np.dot(self.neurons,self.w2)+ self.b2\n",
    "      return output\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "# Positional Encoding (Sinusoidal)\n",
    "# Helping Transformer to understand word order, position information of embeddings.\n",
    "def positional_encoding(sequence_length, embedding_dim):\n",
    "   # Creating position indices (0, 1, 2,... seq_len-1)\n",
    "   position = np.arange(sequence_length)[:, np.newaxis] # Convert to column vector\n",
    "   \n",
    "   # Calculating no. of frequency pairs needed\n",
    "   num_pairs = embedding_dim // 2\n",
    "\n",
    "   # Calculating division term for sinusodial functions\n",
    "   # Creating alternating sine/cosine pattern across dimensions\n",
    "   i = np.arange(num_pairs) # Handles both even & odd patterns\n",
    "   div_term = np.exp(i * (-np.log(10000.0) * 2 / embedding_dim))\n",
    "\n",
    "   # Initializing empty positional encoding matrix\n",
    "   pe = np.zeros((sequence_length, embedding_dim))\n",
    "\n",
    "   # Calculating indices explicitly/clearly/openly\n",
    "   even_indices = 2 * i\n",
    "   odd_indices = 2 * i + 1\n",
    "\n",
    "   # Filling even indices with sine values\n",
    "   pe[:, even_indices] = np.sin(position * div_term) # 0::2 mean start at 0 with stepping size of 2\n",
    "\n",
    "   # Filling odd indices with cosine values\n",
    "   pe[:, odd_indices] = np.cos(position * div_term) # 1::2 mean start at 1 with step size of 2\n",
    "\n",
    "   return pe\n",
    "\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''\n",
    "\n",
    "# LAYER NORMALIZATION\n",
    "# Stablizing network by normalization values in each layer\n",
    "class Layer_Normalization:\n",
    "   def __init__(self, dim):\n",
    "      # Learnable parameters - network will adjust these during training\n",
    "      self.gamma = np.ones(dim) # Scaling factor (intitally no scaling)\n",
    "      self.beta = np.zeros(dim) # Shifting factor (initially no shift)\n",
    "   \n",
    "   def forward(self, input):\n",
    "      # Calculating mean & standard deviation across features/input (last dimension)\n",
    "      mean = input.mean(axis = -1, keepdims = True) # Keeping the dimensions for broadcasting\n",
    "      standard_deviation = input.std(axis = -1, keepdims = True)\n",
    "\n",
    "      # Normalizing: (input - mean) / standard_deviation, & then scale/shift\n",
    "      # 1e-6: 1 x 10^(-6) or 0.000001, which prevents/protects from division by zero\n",
    "      return self.gamma * (input - mean) / (standard_deviation + 1e-6) + self.beta       \n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''   \n",
    "\n",
    "# MODIFIED SELF-ATTENTION BLOCK\n",
    "# Calculating initial attention\n",
    "attention, _ = SelfAttention(Q, K, V)\n",
    "\n",
    "# Adding residual connection (original input) + applying layer normalization\n",
    "# Residual connections help gradients flow through deep networks\n",
    "layer_normalization = Layer_Normalization(embedding_dim)\n",
    "attention = layer_normalization.forward(attention + Embedding) # Original Embedding = residual\n",
    "\n",
    "'''\n",
    "-------------------------------------------------------------------------------------\n",
    "'''   \n",
    "\n",
    "# TRANSFORMER LAYER STACKING\n",
    "# Reapeating/Iterating 6 times as in original paper of Transformer\n",
    "# for _ in range(6):\n",
    "#    # Storing previous output for residual connection\n",
    "#    residual = attention\n",
    "\n",
    "#    # Self Attention using current representation\n",
    "#    attention, _ = SelfAttention(attention, attention, attention)\n",
    "\n",
    "#    # Adding residual connection + normalize\n",
    "#    attention = Layer_Normalization(attention + residual)\n",
    "\n",
    "# Creating stacks of transformer layers\n",
    "transformer_layers = [TransformerLayer(5) for _ in range (6)]\n",
    "\n",
    "# Processing through layers\n",
    "attention = Embedding.copy()\n",
    "for layer in transformer_layers:\n",
    "   attention = layer(attention)\n",
    "\n",
    "\n",
    "# Adding positional encoding to original embedding\n",
    "sequence_length = 1 # Number of token(s)\n",
    "pe = positional_encoding(1,5)\n",
    "# Embedding += positional_encoding(sequence_length, embedding_dim) # Combining content with position info \n",
    "Embedding_Copy = Embedding + pe\n",
    "\n",
    "# Using FFN (with class)\n",
    "ffn = FeedForwardNetwork(input_features = 5, neurons = 2048, output_predicted = 5)\n",
    "ffn_output = ffn.forward_pass(attention)\n",
    "print(\"\\nTransformer Output with class implementation:\\n\",ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 40) (3737641183.py, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[43], line 40\u001b[1;36m\u001b[0m\n\u001b[1;33m    ''''\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 40)\n"
     ]
    }
   ],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "   def __init__(self, embed_size, heads):\n",
    "      super(SelfAttention, self).__init__()\n",
    "      self.embed_size = embed_size # Dimension of input embeddings (e.g., 512)\n",
    "      self.heads= heads # Number of attention heads (e.g., 8)\n",
    "      self.head_dim = embed_size // heads # Dimension per head (e.g., 512/8=64)\n",
    "\n",
    "      # Ensuring embed_size is divisible by the number of heads\n",
    "      assert self.head_dim*heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "\n",
    "      # Linear layers for Q, K, V\n",
    "      # Linear layers to project embeddings into Query (Q), Key (K), Value (V) vectors\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "      # Final linear layer to combine outputs from all heads\n",
    "      self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "   \n",
    "   def forward(self, values, keys, queries, mask=None):\n",
    "      # Get batch size (N) and sequence lengths for values, keys, queries\n",
    "      N = queries.shape[0] #Batch Size\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "      # Spliting embeddings into multiple heads (reshape for parallel computation)\n",
    "      # New shape: (N, seq_len, heads, head_dim)\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      queries = values.reshape(N, query_len, self.heads, self.head_dim)\n",
    "      \n",
    "      # Computing Q, K, V\n",
    "      # Projecting embeddings to Q, K, V using linear layers\n",
    "      values = self.values(values)\n",
    "      keys = self.keys(keys)\n",
    "      queries = self.queries(queries)\n",
    "      ''''\n",
    "      Attention scores: (Q*K^T) / sqrt(d_k)\n",
    "      Compute attention scores (Q * K^T)\n",
    "      Einstein summation: (batch, query_len, heads, head_dim) x (batch, key_len, heads, head_dim)\n",
    "      '''' \n",
    "      # Result shape: (batch, heads, query_len, key_len)\n",
    "      ''''\n",
    "      nqhd, nkhd -> nhqk; notation to describe the interaction of 2 tensors or more.\n",
    "      (nqhd): Queries, (nkhd): keys, (nhqk): Result/Resulting shape\n",
    "\n",
    "      n:\tBatch size (number of sequences)\n",
    "      q:\tQuery sequence length\n",
    "      k:\tKey sequence length\n",
    "      h:\tNumber of attention heads\n",
    "      d:\tDimension per head (head_dim)\n",
    "\n",
    "      - nqhd,nkhd->nhqk computes all pairwise interactions between queries and keys across batches and heads.\n",
    "      - This is the core step in self-attention to determine how words in a sequence relate to each other.\n",
    "      ''''\n",
    "      energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])  \n",
    "\n",
    "      # Applying mask (if provided) to ignore certain positions (e.g., padding or future tokens)\n",
    "      if mask is not None:\n",
    "         # Replace masked positions with -inf\n",
    "         energy = energy.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "      \n",
    "      # Normalizing scores using softmax and scale by sqrt(embed_size) for stability\n",
    "      attention = torch.softmax(energy / (self.embed_size**(0.5)), dim = 3)\n",
    "\n",
    "      # Computing weighted sum of values using attention scores\n",
    "      # Result shape: (batch, query_len, heads, head_dim)\n",
    "      out = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values])\n",
    "      \n",
    "      # Reshaping back to (batch, query_len, embed_size) and pass through final linear layer\n",
    "      out = out.reshape(N, query_len, self.embed_size)\n",
    "      out = self.fc_out(out)\n",
    "      return out\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, embed_size, max_seq_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "\n",
    "      # Creating a matrix of shape (max_seq_len, embed_size) initialized to zeros\n",
    "      pe = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "      # Generating positions from 0 to max_seq_len-1\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "      # Computing divisor term for scaling positional encoding\n",
    "      # Using exp and log to avoid numerical instability\n",
    "      div_term = torch.exp(torch.arange(0, embed_size, 2).float()*(-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "      \n",
    "      # Applying sine to even indices and cosine to odd indices\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # Even positions\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # Odd positions\n",
    "\n",
    "      # Register as a buffer (non-trainable parameter) for saving/loading\n",
    "      self.register_buffer(\"pe\", pe.unsqueeze(0)) # (1, max_seq_len, embed_size)\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      # Add positional encoding to input embeddings\n",
    "      # x shape: (batch, seq_len, embed_size)\n",
    "      # pe shape: (1, max_seq_len, embed_size) â†’ automatically broadcasted\n",
    "      return x + self.pe[:, :x.shape[1], :]\n",
    "         \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block (Encoder Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "   def __init__(self, embed_size, heads, dropout = 0.1):\n",
    "      super(TransformerBlock, self).__init__()\n",
    "\n",
    "      # Multi-head self-attention layer\n",
    "      self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "      # Layer normalization for stabilizing training\n",
    "      self.norm1 = nn.LayerNorm(embed_size)\n",
    "      self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "       # Feed-forward network (expands and contracts embeddings)\n",
    "      self.ff = nn.Sequential(\n",
    "         nn.Linear(embed_size, 4 * embed_size), # Expand to 4 * embed_size\n",
    "         nn.ReLU(), # Non-linearity\n",
    "         nn.Linear(4 * embed_size, embed_size), # Contract back to embed_size\n",
    "      )\n",
    "\n",
    "      # Dropout for regularization\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Compute self-attention\n",
    "      attention = self.attention(x, x, x, mask)\n",
    "\n",
    "      # Step 2: Residual connection + layer norm\n",
    "      x = self.norm1(attention + x) # Residual skip connection\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # Step 3: Feed-forward network\n",
    "      ff = self.ff(x)\n",
    "\n",
    "      # Step 4: Residual connection + layer norm\n",
    "      x = self.norm2(ff + x)\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "   def __init__(self, src_vocab_size, embed_size, num_layers, heads, max_seq_len, dropout = 0.1):\n",
    "      super(Transformer, self).__init__()\n",
    "\n",
    "      # Embedding layer to convert token IDs to vectors\n",
    "      self.embed = nn.Embedding(src_vocab_size, embed_size)\n",
    "\n",
    "      # Positional encoding to add sequence information\n",
    "      self.pe = PositionalEncoding(embed_size, max_seq_len)\n",
    "\n",
    "      # Stack multiple transformer blocks (encoder layers)\n",
    "      self.layers = nn.ModuleList([TransformerBlock(embed_size, heads, dropout)\n",
    "      for _ in range (num_layers)\n",
    "      ])\n",
    "\n",
    "      # Final linear layer to project embeddings back to vocabulary size\n",
    "      self.fc_out = nn.Linear(embed_size, src_vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Convert token IDs to embeddings\n",
    "      x = self.embed(x) # (batch, seq_len) â†’ (batch, seq_len, embed_size)\n",
    "\n",
    "      # Step 2: Add positional encoding\n",
    "      x = self.pe(x)\n",
    "\n",
    "      # Step 3: Pass through each transformer block\n",
    "      for layer in self.layers:\n",
    "         x = layer(x,mask)\n",
    "\n",
    "         # Step 4: Project embeddings to vocabulary logits\n",
    "      x = self.fc_out(x) # (batch, seq_len, vocab_size)\n",
    "      return x\n",
    "          \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Transformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m vocab_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m \u001b[39m# Vocabulary size (e.g., 10 tokens: 0-9)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39m# Initializing model, loss, and optimizer\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m model \u001b[39m=\u001b[39m Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n\u001b[0;32m     10\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss() \u001b[39m# For classification tasks\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Transformer' is not defined"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128 # Dimension of embeddings\n",
    "heads = 8 # Number of attention heads\n",
    "num_layers = 3 # Number of transformer blocks\n",
    "max_seq_len = 10 # Maximum sequence length\n",
    "vocab_size = 10 # Vocabulary size (e.g., 10 tokens: 0-9)\n",
    "\n",
    "# Initializing model, loss, and optimizer\n",
    "model = Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n",
    "criterion = nn.CrossEntropyLoss() # For classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Generating toy data (input and target are the same for a copy task)\n",
    "src = torch.randint(0, vocab_size, (32, max_seq_len)) # Fake input (batch_size=32)\n",
    "trg = src.clone() # Target is same as input (simple copy task)\n",
    "\n",
    "\n",
    "# Traning loop \n",
    "print(\"Transformer Output:\\n\")\n",
    "for epoch in range(100):\n",
    "   # Forward pass: compute model predictions\n",
    "   output = model(src) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "   # Compute loss (flatten batch and sequence dimensions for cross-entropy)\n",
    "   loss = criterion(output.view(-1, vocab_size), trg.view(-1))\n",
    "\n",
    "   # Backpropagation\n",
    "   optimizer.zero_grad() # Clear gradients\n",
    "   loss.backward() # Compute gradients\n",
    "   optimizer.step() # Update weights\n",
    "   print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
