{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Singh | [LinkedIn](https://www.linkedin.com/in/adityasingh2022/) | [GitHub](https://github.com/adityasinghcoding)\n",
    "\n",
    "# __Transformer from Scratch__\n",
    "\n",
    "`Transformer`(transform/translate) helps in understanding real world meaning of the data by transforming the relations(important/priority-wise) w.r.t actual fundamental meaning. \n",
    "<br>___Note :___ Transformer acts as the model inside main model.\n",
    "\n",
    "---\n",
    "### __Components of Transformer__\n",
    "1. ___Encoder Layer___\n",
    "   - Input Embeddings\n",
    "   - Positional Encoding\n",
    "   - Self-Attention Layer\n",
    "   - Feed Forward Network\n",
    "2. ___Decoder Layer___\n",
    "   - Masked Self-Attention\n",
    "   - Encoder-Decoder Attention\n",
    "   - Feed-Forward Network\n",
    "\n",
    "---\n",
    "**Attention**: It finds, measure, evaluate the relation of one word or any data with other data present in the batch/sequence/matrix. \n",
    "There are 2 types of Attentions:\n",
    "- **Self Attention**: Evaluate connection/relation of each word/data. \n",
    "- **Multi Head Attention**: Multiple Self Attentions in parallel, to evaluate complex relations between entities(data).\n",
    "--- \n",
    "### __Architecture of Transformers__\n",
    "#### __Encoder Layer__\n",
    "- **Input** (Tokens/Embedding are the raw input to the Transformer.)\n",
    "   Token's embedding transformed/split into 3 vectors: Q, K, V. Each token has its own Q, K, V.\n",
    "- **Positional Encoding** (Metadata of data order with sine/cosine functions)\n",
    "- **Self Attention Layer** (Relation capturing & evaluation or in technical terms: establishing weights)\n",
    "   - $Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ <br>_Softmax(Sum to 1) is used to normalize the data(scores) & to convert scores to probability distribution._\n",
    "      - _Q, K, V:_ Query, Key, Value\n",
    "      - ${QK^T}$: Compute Scores/Interaction with others. <br>_Represents the strength of attention._\n",
    "      - _V_: Value vectors, Dynamic dictionary. <br>_It contains content/dictionary/data which is linked with Q & K through multiplication helping in translating the relation in final understanding/output(placement of word). V helps in understanding the context of the language._\n",
    "   - $\\sqrt{d_k}$: Fixes large dot products causing gradient issues.<br>___\"d\"___ is the dimensionality.\n",
    "- __Feed-Forward Network__ (Simple Neural Network)\n",
    "\n",
    "#### __Decoder Layer__\n",
    "- __Masked-Self Attention__ (Hiding future tokens during training)\n",
    "- __Encoder-Decoder Attention__ (___Q___ from Decoder, ___K & V___ from Encoder. Encoder output used to focus on relevant input parts)\n",
    "---\n",
    "### How __Q, K, V__ Works in Transformers:\n",
    "   1. __Input Tokens__ → Projected into __Keys (K)__ and __Values (V)__.\n",
    "      - Think: Each token writes its __data (V)__ and a __label (K)__ into a dictionary.\n",
    "   2. __Queries (Q)__ \"search\" this dictionary by comparing __Q__ to all __Ks__.\n",
    "   3. The best-matching __Ks__ (highest attention scores) retrieve their corresponding __Vs__.\n",
    "   4. The output is a __weighted blend__ of the retrieved __Vs__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "   def __init__(self, embed_size, heads):\n",
    "      super(SelfAttention, self).__init__()\n",
    "      self.embed_size = embed_size # Dimension of input embeddings (e.g., 512)\n",
    "      self.heads= heads # Number of attention heads (e.g., 8)\n",
    "      self.head_dim = embed_size // heads # Dimension per head (e.g., 512/8=64)\n",
    "\n",
    "      # Ensuring embed_size is divisible by the number of heads\n",
    "      assert self.head_dim*heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "\n",
    "      # Linear layers for Q, K, V\n",
    "      # Linear layers to project embeddings into Query (Q), Key (K), Value (V) vectors\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "\n",
    "      # Final linear layer to combine outputs from all heads\n",
    "      self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "   \n",
    "   def forward(self, values, keys, queries, mask=None):\n",
    "      # Get batch size (N) and sequence lengths for values, keys, queries\n",
    "      N = queries.shape[0] #Batch Size\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "      # Spliting embeddings into multiple heads (reshape for parallel computation)\n",
    "      # New shape: (N, seq_len, heads, head_dim)\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      queries = values.reshape(N, query_len, self.heads, self.head_dim)\n",
    "      \n",
    "      # Computing Q, K, V\n",
    "      # Projecting embeddings to Q, K, V using linear layers\n",
    "      values = self.values(values)\n",
    "      keys = self.keys(keys)\n",
    "      queries = self.queries(queries)\n",
    "''''\n",
    "      Attention scores: (Q*K^T) / sqrt(d_k)\n",
    "      Compute attention scores (Q * K^T)\n",
    "      Einstein summation: (batch, query_len, heads, head_dim) x (batch, key_len, heads, head_dim)\n",
    "'''  \n",
    "      # Result shape: (batch, heads, query_len, key_len)\n",
    "      '''''\n",
    "      nqhd, nkhd -> nhqk; notation to describe the interaction of 2 tensors or more.\n",
    "      (nqhd): Queries, (nkhd): keys, (nhqk): Result/Resulting shape\n",
    "\n",
    "      n:\tBatch size (number of sequences)\n",
    "      q:\tQuery sequence length\n",
    "      k:\tKey sequence length\n",
    "      h:\tNumber of attention heads\n",
    "      d:\tDimension per head (head_dim)\n",
    "\n",
    "      - nqhd,nkhd->nhqk computes all pairwise interactions between queries and keys across batches and heads.\n",
    "      - This is the core step in self-attention to determine how words in a sequence relate to each other.\n",
    "      '''''\n",
    "      energy = torch.einsum(\"nqhd, nkhd->nhqk\", [queries, keys])  \n",
    "\n",
    "      # Applying mask (if provided) to ignore certain positions (e.g., padding or future tokens)\n",
    "      if mask is not None:\n",
    "         # Replace masked positions with -inf\n",
    "         energy = energy.masked_fill(mask == 0, float(\"-1e20\")) \n",
    "      \n",
    "      # Normalizing scores using softmax and scale by sqrt(embed_size) for stability\n",
    "      attention = torch.softmax(energy / (self.embed_size**(0.5)), dim = 3)\n",
    "\n",
    "      # Computing weighted sum of values using attention scores\n",
    "      # Result shape: (batch, query_len, heads, head_dim)\n",
    "      out = torch.einsum(\"nhql, nlhd->nqhd\", [attention, values])\n",
    "      \n",
    "      # Reshaping back to (batch, query_len, embed_size) and pass through final linear layer\n",
    "      out = out.reshape(N, query_len, self.embed_size)\n",
    "      out = self.fc_out(out)\n",
    "      return out\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, embed_size, max_seq_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "\n",
    "      # Creating a matrix of shape (max_seq_len, embed_size) initialized to zeros\n",
    "      pe = torch.zeros(max_seq_len, embed_size)\n",
    "\n",
    "      # Generating positions from 0 to max_seq_len-1\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "      # Computing divisor term for scaling positional encoding\n",
    "      # Using exp and log to avoid numerical instability\n",
    "      div_term = torch.exp(torch.arange(0, embed_size, 2).float()*(-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "      \n",
    "      # Applying sine to even indices and cosine to odd indices\n",
    "      pe[:, 0::2] = torch.sin(position * div_term) # Even positions\n",
    "      pe[:, 1::2] = torch.cos(position * div_term) # Odd positions\n",
    "\n",
    "      # Register as a buffer (non-trainable parameter) for saving/loading\n",
    "      self.register_buffer(\"pe\", pe.unsqueeze(0)) # (1, max_seq_len, embed_size)\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "      # Add positional encoding to input embeddings\n",
    "      # x shape: (batch, seq_len, embed_size)\n",
    "      # pe shape: (1, max_seq_len, embed_size) → automatically broadcasted\n",
    "      return x + self.pe[:, :x.shape[1], :]\n",
    "         \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block (Encoder Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "   def __init__(self, embed_size, heads, dropout = 0.1):\n",
    "      super(TransformerBlock, self).__init__()\n",
    "\n",
    "      # Multi-head self-attention layer\n",
    "      self.attention = SelfAttention(embed_size, heads)\n",
    "\n",
    "      # Layer normalization for stabilizing training\n",
    "      self.norm1 = nn.LayerNorm(embed_size)\n",
    "      self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "       # Feed-forward network (expands and contracts embeddings)\n",
    "      self.ff = nn.Sequential(\n",
    "         nn.Linear(embed_size, 4 * embed_size), # Expand to 4 * embed_size\n",
    "         nn.ReLU(), # Non-linearity\n",
    "         nn.Linear(4 * embed_size, embed_size), # Contract back to embed_size\n",
    "      )\n",
    "\n",
    "      # Dropout for regularization\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "      \n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Compute self-attention\n",
    "      attention = self.attention(x, x, x, mask)\n",
    "\n",
    "      # Step 2: Residual connection + layer norm\n",
    "      x = self.norm1(attention + x) # Residual skip connection\n",
    "      x = self.dropout(x)\n",
    "\n",
    "      # Step 3: Feed-forward network\n",
    "      ff = self.ff(x)\n",
    "\n",
    "      # Step 4: Residual connection + layer norm\n",
    "      x = self.norm2(ff + x)\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "   def __init__(self, src_vocab_size, embed_size, num_layers, heads, max_seq_len, dropout = 0.1):\n",
    "      super(Transformer, self).__init__()\n",
    "\n",
    "      # Embedding layer to convert token IDs to vectors\n",
    "      self.embed = nn.Embedding(src_vocab_size, embed_size)\n",
    "\n",
    "      # Positional encoding to add sequence information\n",
    "      self.pe = PositionalEncoding(embed_size, max_seq_len)\n",
    "\n",
    "      # Stack multiple transformer blocks (encoder layers)\n",
    "      self.layers = nn.ModuleList([TransformerBlock(embed_size, heads, dropout)\n",
    "      for _ in range (num_layers)\n",
    "      ])\n",
    "\n",
    "      # Final linear layer to project embeddings back to vocabulary size\n",
    "      self.fc_out = nn.Linear(embed_size, src_vocab_size)\n",
    "\n",
    "\n",
    "   def forward(self, x, mask = None):\n",
    "      # Step 1: Convert token IDs to embeddings\n",
    "      x = self.embed(x) # (batch, seq_len) → (batch, seq_len, embed_size)\n",
    "\n",
    "      # Step 2: Add positional encoding\n",
    "      x = self.pe(x)\n",
    "\n",
    "      # Step 3: Pass through each transformer block\n",
    "      for layer in self.layers:\n",
    "         x = layer(x,mask)\n",
    "\n",
    "         # Step 4: Project embeddings to vocabulary logits\n",
    "      x = self.fc_out(x) # (batch, seq_len, vocab_size)\n",
    "      return x\n",
    "          \n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Toy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.5696732997894287\n",
      "Epoch 1, Loss: 1.7917839288711548\n",
      "Epoch 2, Loss: 1.2647374868392944\n",
      "Epoch 3, Loss: 0.808722972869873\n",
      "Epoch 4, Loss: 0.49040308594703674\n",
      "Epoch 5, Loss: 0.2886057496070862\n",
      "Epoch 6, Loss: 0.18581750988960266\n",
      "Epoch 7, Loss: 0.1287505030632019\n",
      "Epoch 8, Loss: 0.09626717120409012\n",
      "Epoch 9, Loss: 0.07264979183673859\n",
      "Epoch 10, Loss: 0.0587611086666584\n",
      "Epoch 11, Loss: 0.04901111125946045\n",
      "Epoch 12, Loss: 0.041532378643751144\n",
      "Epoch 13, Loss: 0.03541068360209465\n",
      "Epoch 14, Loss: 0.03220308572053909\n",
      "Epoch 15, Loss: 0.028758510947227478\n",
      "Epoch 16, Loss: 0.025462988764047623\n",
      "Epoch 17, Loss: 0.022908825427293777\n",
      "Epoch 18, Loss: 0.021407118067145348\n",
      "Epoch 19, Loss: 0.020005421712994576\n",
      "Epoch 20, Loss: 0.019357429817318916\n",
      "Epoch 21, Loss: 0.0171327143907547\n",
      "Epoch 22, Loss: 0.01600380800664425\n",
      "Epoch 23, Loss: 0.015178951434791088\n",
      "Epoch 24, Loss: 0.01447419822216034\n",
      "Epoch 25, Loss: 0.013669110834598541\n",
      "Epoch 26, Loss: 0.013136759400367737\n",
      "Epoch 27, Loss: 0.012263762764632702\n",
      "Epoch 28, Loss: 0.012485763058066368\n",
      "Epoch 29, Loss: 0.011173472739756107\n",
      "Epoch 30, Loss: 0.010833089239895344\n",
      "Epoch 31, Loss: 0.010386986657977104\n",
      "Epoch 32, Loss: 0.010101916268467903\n",
      "Epoch 33, Loss: 0.009933734312653542\n",
      "Epoch 34, Loss: 0.009589338675141335\n",
      "Epoch 35, Loss: 0.009017912670969963\n",
      "Epoch 36, Loss: 0.008937953040003777\n",
      "Epoch 37, Loss: 0.008543445728719234\n",
      "Epoch 38, Loss: 0.008419708348810673\n",
      "Epoch 39, Loss: 0.00848926231265068\n",
      "Epoch 40, Loss: 0.008036739192903042\n",
      "Epoch 41, Loss: 0.00790367741137743\n",
      "Epoch 42, Loss: 0.007313180714845657\n",
      "Epoch 43, Loss: 0.007564245257526636\n",
      "Epoch 44, Loss: 0.0072868093848228455\n",
      "Epoch 45, Loss: 0.006909646093845367\n",
      "Epoch 46, Loss: 0.0067537338472902775\n",
      "Epoch 47, Loss: 0.00687939440831542\n",
      "Epoch 48, Loss: 0.006463751196861267\n",
      "Epoch 49, Loss: 0.006576483603566885\n",
      "Epoch 50, Loss: 0.006403317209333181\n",
      "Epoch 51, Loss: 0.0063038961961865425\n",
      "Epoch 52, Loss: 0.005999045446515083\n",
      "Epoch 53, Loss: 0.006123571190983057\n",
      "Epoch 54, Loss: 0.006088731344789267\n",
      "Epoch 55, Loss: 0.005796578712761402\n",
      "Epoch 56, Loss: 0.005773275159299374\n",
      "Epoch 57, Loss: 0.00573262432590127\n",
      "Epoch 58, Loss: 0.005675707943737507\n",
      "Epoch 59, Loss: 0.005386894568800926\n",
      "Epoch 60, Loss: 0.005261418409645557\n",
      "Epoch 61, Loss: 0.00508586410433054\n",
      "Epoch 62, Loss: 0.005288970656692982\n",
      "Epoch 63, Loss: 0.00531377037987113\n",
      "Epoch 64, Loss: 0.005129582714289427\n",
      "Epoch 65, Loss: 0.005224450491368771\n",
      "Epoch 66, Loss: 0.005019595846533775\n",
      "Epoch 67, Loss: 0.004834051243960857\n",
      "Epoch 68, Loss: 0.004935932345688343\n",
      "Epoch 69, Loss: 0.004666364751756191\n",
      "Epoch 70, Loss: 0.0047940900549292564\n",
      "Epoch 71, Loss: 0.004790388513356447\n",
      "Epoch 72, Loss: 0.0044948444701731205\n",
      "Epoch 73, Loss: 0.004445356782525778\n",
      "Epoch 74, Loss: 0.0044010477140545845\n",
      "Epoch 75, Loss: 0.004296580795198679\n",
      "Epoch 76, Loss: 0.004305583890527487\n",
      "Epoch 77, Loss: 0.004218386486172676\n",
      "Epoch 78, Loss: 0.004366791807115078\n",
      "Epoch 79, Loss: 0.004182284232228994\n",
      "Epoch 80, Loss: 0.004086346365511417\n",
      "Epoch 81, Loss: 0.0044137267395854\n",
      "Epoch 82, Loss: 0.0040343813598155975\n",
      "Epoch 83, Loss: 0.004024915862828493\n",
      "Epoch 84, Loss: 0.0040013836696743965\n",
      "Epoch 85, Loss: 0.003972864244133234\n",
      "Epoch 86, Loss: 0.003840117948129773\n",
      "Epoch 87, Loss: 0.0037880619056522846\n",
      "Epoch 88, Loss: 0.003670876380056143\n",
      "Epoch 89, Loss: 0.0038130159955471754\n",
      "Epoch 90, Loss: 0.0035945873241871595\n",
      "Epoch 91, Loss: 0.003720305860042572\n",
      "Epoch 92, Loss: 0.0037287131417542696\n",
      "Epoch 93, Loss: 0.0035793916322290897\n",
      "Epoch 94, Loss: 0.00363504933193326\n",
      "Epoch 95, Loss: 0.003457580925896764\n",
      "Epoch 96, Loss: 0.0035108490847051144\n",
      "Epoch 97, Loss: 0.003498681355267763\n",
      "Epoch 98, Loss: 0.0035033547319471836\n",
      "Epoch 99, Loss: 0.0034919101744890213\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "embed_size = 128 # Dimension of embeddings\n",
    "heads = 8 # Number of attention heads\n",
    "num_layers = 3 # Number of transformer blocks\n",
    "max_seq_len = 10 # Maximum sequence length\n",
    "vocab_size = 10 # Vocabulary size (e.g., 10 tokens: 0-9)\n",
    "\n",
    "# Initializing model, loss, and optimizer\n",
    "model = Transformer(vocab_size, embed_size, num_layers, heads, max_seq_len)\n",
    "criterion = nn.CrossEntropyLoss() # For classification tasks\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr= 0.001)\n",
    "\n",
    "# Generating toy data (input and target are the same for a copy task)\n",
    "src = torch.randint(0, vocab_size, (32, max_seq_len)) # Fake input (batch_size=32)\n",
    "trg = src.clone() # Target is same as input (simple copy task)\n",
    "\n",
    "\n",
    "# Traning loop \n",
    "for epoch in range(100):\n",
    "   # Forward pass: compute model predictions\n",
    "   output = model(src) # Shape: (batch, seq_len, vocab_size)\n",
    "\n",
    "   # Compute loss (flatten batch and sequence dimensions for cross-entropy)\n",
    "   loss = criterion(output.view(-1, vocab_size), trg.view(-1))\n",
    "\n",
    "   # Backpropagation\n",
    "   optimizer.zero_grad() # Clear gradients\n",
    "   loss.backward() # Compute gradients\n",
    "   optimizer.step() # Update weights\n",
    "   print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
