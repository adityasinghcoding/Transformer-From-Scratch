{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name: Aditya Singh | [LinkedIn](https://www.linkedin.com/in/adityasingh2022/) | [GitHub](https://github.com/adityasinghcoding)\n",
    "\n",
    "# __Transformer from Scratch__\n",
    "\n",
    "`Transformer`(transform/translate) helps in understanding real world meaning of the data by transforming the relations(important/priority-wise) w.r.t actual fundamental meaning. \n",
    "<br>___Note :___ Transformer acts as the model inside main model.\n",
    "\n",
    "---\n",
    "### __Components of Transformer__\n",
    "1. ___Encoder Layer___\n",
    "   - Input Embeddings\n",
    "   - Positional Encoding\n",
    "   - Self-Attention Layer\n",
    "   - Feed Forward Network\n",
    "2. ___Decoder Layer___\n",
    "   - Masked Self-Attention\n",
    "   - Encoder-Decoder Attention\n",
    "   - Feed-Forward Network\n",
    "\n",
    "---\n",
    "**Attention**: It finds, measure, evaluate the relation of one word or any data with other data present in the batch/sequence/matrix. \n",
    "There are 2 types of Attentions:\n",
    "- **Self Attention**: Evaluate connection/relation of each word/data. \n",
    "- **Multi Head Attention**: Multiple Self Attentions in parallel, to evaluate complex relations between entities(data).\n",
    "--- \n",
    "### __Architecture of Transformers__\n",
    "#### __Encoder Layer__\n",
    "- **Input** (Data in Vector; Embedding of words)\n",
    "- **Positional Encoding** (Metadata of data order with sine/cosine functions)\n",
    "- **Self Attention Layer** (Relation capturing & evaluation or in technical terms: establishing weights)\n",
    "   - $Attention(Q,K,V) = softmax\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$ <br>_Softmax(Sum to 1) is used to normalize the data(scores)._\n",
    "      - _Q, K, V:_ Query, Key, Value\n",
    "      - ${QK^T}$: Compute Scores/Interaction with others.\n",
    "      - V: Vectors. \n",
    "   - $\\sqrt{d_k}$: Fixes large dot products causing gradient issues.\n",
    "- __Feed-Forward Network__ (Simple Neural Network)\n",
    "\n",
    "#### __Decoder Layer__\n",
    "- __Masked-Self Attention__ (Hiding future tokens during training)\n",
    "- __Encoder-Decoder Attention__ (___Q___ from Decoder, ___K & V___ from Encoder. Encoder output used to focus on relevant input parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "   def __init__(self, embed_size, heads):\n",
    "      super(SelfAttention, Self).__init__()\n",
    "      self.embed_size = embed_size\n",
    "      self.heads= heads\n",
    "      self.head_dim = embed_size // heads\n",
    "\n",
    "      assert self.head_dim*heads == embed_size, \"Embed size must be divisible by heads\"\n",
    "\n",
    "      # Linear layers for Q, K, V\n",
    "      self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "      self.fc_out = nn.Linear(embed_size, embed_size)\n",
    "   \n",
    "   def forward(self, values, keys, queries, mask=None):\n",
    "      N = queries.shape[0] #Batch Size\n",
    "      value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "\n",
    "      # Split embeddings into heads\n",
    "      values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "      keys = values.reshape(N, key_len, self.heads, self.head_dim)\n",
    "      queries = values.reshape(N, query_len, self.heads, self.head_dim)\n",
    "      \n",
    "      # Computing Q, K, V\n",
    "      values = self.values(values)\n",
    "      keys = self.keys(keys)\n",
    "      queries = self.queries(queries)\n",
    "\n",
    "      # Attention scores: (Q*K^T) / sqrt(d_k)\n",
    "      energy = torch.eisum(\"nhql, nlhd->nqhd\", [attention, values]).reshape(N, query_len, self.embed_size)\n",
    "      out = self.fc_out(out)\n",
    "      return out\n",
    "       "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "   def __init__(self, embed_size, max_seq_len):\n",
    "      super(PositionalEncoding, self).__init__()\n",
    "      pe = torch.zeros(max_seq_len, embed_size)\n",
    "      position = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "      div_term = torch.exp(torch.arange(0, embed_size, 2).float()*(-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "\n",
    "      pe[:, 0::2] = torch.sin(position * div_term)\n",
    "      pe[:, 1::2] = torch.cos(position * div_term)\n",
    "      self.register_buffer(\"pe\", pe.unsqueeze(0)) # (1, max_seq_len, embed_size)\n",
    "\n",
    "\n",
    "      def forward(self, x):\n",
    "         return x + self.pe[:, :x.shape[1], :]\n",
    "         \n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block (Encoder Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "   def __init__(self, embed_size, heads, dropout = 0.1):\n",
    "      super(TransformerBlock, self).__init__()\n",
    "      self.attention = SelfAttention(embed_size, heads)\n",
    "      self.norm1 = nn.LayerNorm(embed_size)\n",
    "      self.norm2 = nn.LayerNorm(embed_size)\n",
    "      self.ff = nn.Sequential(\n",
    "         nn.Linear(embed_size, 4 * embed_size),\n",
    "         nn.ReLU(),\n",
    "         nn.Linear(4 * embed_size, embed_size),\n",
    "      )\n",
    "      self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "      \n",
    "      def forward(self, x, mask = None):\n",
    "         attention = self.attention(x, x, x, mask)\n",
    "         x = self.norm1(attention + x)\n",
    "         x = self.dropout(x)\n",
    "         ff = self.ff(x)\n",
    "         x = self.norm2(ff + x)\n",
    "         x = self.dropout(x)\n",
    "         return x\n",
    "          "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Transformer (Encoder-Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
